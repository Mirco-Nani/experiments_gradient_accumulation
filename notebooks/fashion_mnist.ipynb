{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow/datasets",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mirco-Nani/experiments_gradient_accumulation/blob/main/notebooks/fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSV_OlCFKOD"
      },
      "source": [
        "# Training a neural network on fashion MNIST with Keras\n",
        "\n",
        "This simple example demonstrate how to plug TFDS into a Keras model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8y9ZkLXmAZc"
      },
      "source": [
        "Copyright 2020 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGw9EgE0tC0C"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/datasets/keras_example\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/datasets/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTBSvHcSLBzc"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afsd0NnRtnUZ"
      },
      "source": [
        "np.ceil(3.2).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd8D4oY617G8"
      },
      "source": [
        "class DsInfo:\n",
        "  def __init__(self, ds, batch_size, num_examples):\n",
        "    self.ds=ds\n",
        "    self.batch_size=batch_size\n",
        "    self.num_examples = num_examples\n",
        "    \n",
        "  def num_batches(self):\n",
        "    return np.ceil(self.num_examples/self.batch_size).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "  plt.figure(figsize=(20,10))\n",
        "\n",
        "  plt.subplot(221)\n",
        "  plt.plot(history['loss'])\n",
        "  plt.plot(history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'])\n",
        "\n",
        "  plt.subplot(222)\n",
        "  plt.plot(history['sparse_categorical_accuracy'])\n",
        "  plt.plot(history['val_sparse_categorical_accuracy'])\n",
        "  plt.title('model sparse_categorical_accuracy')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def prepare_ds(batch_size=128, cache=True, shuffle=True):\n",
        "  def normalize_img(image, label):\n",
        "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "    return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "  (ds_train, ds_test), ds_info = tfds.load(\n",
        "      'fashion_mnist',\n",
        "      split=['train', 'test'],\n",
        "      shuffle_files=shuffle,\n",
        "      as_supervised=True,\n",
        "      with_info=True,\n",
        "  )\n",
        "\n",
        "  ds_train = ds_train.map(\n",
        "      normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  if cache: ds_train = ds_train.cache()\n",
        "  if shuffle: ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "  ds_train = ds_train.batch(batch_size)\n",
        "  ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds_test = ds_test.batch(128)\n",
        "  if cache: ds_test = ds_test.cache()\n",
        "  ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return (\n",
        "      DsInfo(ds_train, batch_size, ds_info.splits['train'].num_examples),\n",
        "      DsInfo(ds_test, batch_size, ds_info.splits['test'].num_examples)\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "def build_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128,activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "def build_and_train(ds_train, ds_test, epochs):\n",
        "  model = build_model()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        "  )\n",
        "\n",
        "  history = model.fit(\n",
        "      ds_train.ds,\n",
        "      epochs=6,\n",
        "      validation_data=ds_test.ds,\n",
        "  )\n",
        "\n",
        "  return history.history\n",
        "\n",
        "\n",
        "def build_and_custom_train(ds_train, ds_test, epochs):\n",
        "  model = build_model()\n",
        "\n",
        "  optimizer=tf.keras.optimizers.Adam(0.001)\n",
        "  loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy=tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  \n",
        "  history={'loss':[], 'val_loss':[], 'sparse_categorical_accuracy':[], 'val_sparse_categorical_accuracy':[]}\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    losses, accuracies = [], []\n",
        "    val_losses, val_accuracies = [], []\n",
        "    train = ds_train.ds\n",
        "    valid = ds_test.ds\n",
        "    accuracy.reset_state()\n",
        "    for step, (images, labels) in tqdm(enumerate(train), desc=f\"Training epoch {epoch}\", total=ds_train.num_batches()):\n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "        logits=model(images)\n",
        "        loss=loss_function(labels, logits)\n",
        "      gradients = tape.gradient(loss, model.trainable_weights)\n",
        "      del tape #?\n",
        "\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "      step_loss = loss.numpy()\n",
        "      losses.append(step_loss)\n",
        "\n",
        "      accuracy.update_state(labels, logits)\n",
        "    \n",
        "    history['loss'].append(sum(losses) / len(losses))\n",
        "    history['sparse_categorical_accuracy'].append(accuracy.result().numpy())\n",
        "\n",
        "    accuracy.reset_state()\n",
        "    for step, (images, labels) in tqdm(enumerate(valid), desc=f\"Validating epoch {epoch}\", total=ds_test.num_batches()):\n",
        "      logits=model(images)\n",
        "      val_losses.append(loss_function(labels, logits).numpy())\n",
        "      accuracy.update_state(labels, logits)\n",
        "      val_accuracies.append(accuracy.result().numpy())\n",
        "\n",
        "    history['val_loss'].append(sum(val_losses) / len(val_losses))\n",
        "    history['val_sparse_categorical_accuracy'].append(sum(val_accuracies) / len(val_accuracies))\n",
        "  \n",
        "  return history\n",
        "    \n",
        "\n",
        "def build_and_gradient_accumulation_train(ds_train, ds_test, epochs, accumulation_steps):\n",
        "  model = build_model()\n",
        "\n",
        "  optimizer=tf.keras.optimizers.Adam(0.001)\n",
        "  loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  accuracy=tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  \n",
        "  history={'loss':[], 'val_loss':[], 'sparse_categorical_accuracy':[], 'val_sparse_categorical_accuracy':[]}\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    losses, accuracies = [], []\n",
        "    val_losses, val_accuracies = [], []\n",
        "    train = ds_train.ds\n",
        "    valid = ds_test.ds\n",
        "    accuracy.reset_state()\n",
        "\n",
        "    for step, (images, labels) in tqdm(enumerate(train), desc=f\"Training epoch {epoch}\", total=ds_train.num_batches()):\n",
        "      acc_step_size = int(ds_train.batch_size/accumulation_steps)\n",
        "      acc_gradients = [tf.zeros_like(v) for v in model.trainable_weights]\n",
        "      num_gradients=0\n",
        "      for acc_step in tqdm(range(0, accumulation_steps, acc_step_size), leave=False):\n",
        "        acc_images, acc_labels = images[acc_step:acc_step+acc_step_size], labels[acc_step:acc_step+acc_step_size]\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "          logits=model(acc_images)\n",
        "          loss=loss_function(acc_labels, logits)\n",
        "        gradient = tape.gradient(loss, model.trainable_weights)\n",
        "        acc_gradients = [(acum_grad+grad) for acum_grad, grad in zip(acc_gradients, gradient)]\n",
        "        num_gradients += 1\n",
        "        del tape \n",
        "\n",
        "      gradients = [grad/num_gradients for grad in acc_gradients]\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "      step_loss = loss.numpy()\n",
        "      losses.append(step_loss)\n",
        "\n",
        "      accuracy.update_state(labels, logits)\n",
        "    \n",
        "    history['loss'].append(sum(losses) / len(losses))\n",
        "    history['sparse_categorical_accuracy'].append(accuracy.result().numpy())\n",
        "\n",
        "    accuracy.reset_state()\n",
        "    for step, (images, labels) in tqdm(enumerate(valid), desc=f\"Validating epoch {epoch}\", total=ds_test.num_batches()):\n",
        "      logits=model(images)\n",
        "      val_losses.append(loss_function(labels, logits).numpy())\n",
        "      accuracy.update_state(labels, logits)\n",
        "      val_accuracies.append(accuracy.result().numpy())\n",
        "\n",
        "    history['val_loss'].append(sum(val_losses) / len(val_losses))\n",
        "    history['val_sparse_categorical_accuracy'].append(sum(val_accuracies) / len(val_accuracies))\n",
        "  \n",
        "  return history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def task(batch_size=128, cache_ds=True, epochs=6, deterministic=True):\n",
        "  if deterministic:\n",
        "    tf.random.set_seed(1234)\n",
        "  ds_train, ds_test = prepare_ds(batch_size, cache_ds, shuffle=(not deterministic))\n",
        "  history=build_and_train(ds_train, ds_test, epochs)\n",
        "  plot_history(history)\n",
        "\n",
        "\n",
        "def custom_task(batch_size=128, cache_ds=True, epochs=6, deterministic=True):\n",
        "  if deterministic:\n",
        "    tf.random.set_seed(1234)\n",
        "  ds_train, ds_test = prepare_ds(batch_size, cache_ds, shuffle=(not deterministic))\n",
        "  history=build_and_custom_train(ds_train, ds_test, epochs)\n",
        "  plot_history(history)\n",
        "\n",
        "\n",
        "def gradient_accumulation_task(batch_size=128, cache_ds=True, epochs=6, deterministic=True, accumulation_steps=1):\n",
        "  if deterministic:\n",
        "    tf.random.set_seed(1234)\n",
        "  ds_train, ds_test = prepare_ds(batch_size, cache_ds, shuffle=(not deterministic))\n",
        "  history=build_and_gradient_accumulation_train(ds_train, ds_test, epochs, accumulation_steps)\n",
        "  plot_history(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bB2GrP_9i_J"
      },
      "source": [
        "task()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUr_2mbDmWoo"
      },
      "source": [
        "task()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHztYGty9x7D"
      },
      "source": [
        "task(cache_ds=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WNR8bHr90cM"
      },
      "source": [
        "task(batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id9tot1D-O31"
      },
      "source": [
        "custom_task()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlQj9HQqxw4b"
      },
      "source": [
        "gradient_accumulation_task()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNuo8Bdf5qZM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBRNUFjpDNNo"
      },
      "source": [
        "# tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0YeOG-VDJPi"
      },
      "source": [
        "ds_train, ds_test = prepare_ds()\n",
        "model = build_model()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train.ds,\n",
        "    epochs=6,\n",
        "    validation_data=ds_test.ds,\n",
        ")\n",
        "\n",
        "plot_history(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfLpTSaGDK1G"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtG7_et-D7nN"
      },
      "source": [
        "model.inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwphejlE4lgV"
      },
      "source": [
        "model.trainable_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrROeFcg4oh1"
      },
      "source": [
        "model.trainable_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvpNCuogEZBm"
      },
      "source": [
        " ds_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yizh15XEd6F"
      },
      "source": [
        "train = iter(ds_train)\n",
        "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "for step, (images, labels) in enumerate(train):\n",
        "  print((images.shape, labels.shape))\n",
        "  break\n",
        "logits=model(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u5YmFwsGG3x"
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2XyD-XcEpHu"
      },
      "source": [
        "loss(labels, logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmAHd15kFSu9"
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gohPCG77G2kv"
      },
      "source": [
        "metric=tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4iU86ZVIIRU"
      },
      "source": [
        "metric(labels, logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDFgg8DSIO9H"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.array([2,3]).prod()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5tZQXE4oU5a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}